2025-11-30 17:53:44,895 - INFO - Loading tokenizer and model: meta-llama/Llama-3.2-1B-Instruct
2025-11-30 17:53:46,796 - INFO - Tokenizer lacked pad token; set to eos_token.
2025-11-30 17:53:46,797 - INFO - Loading reference model...
2025-11-30 17:53:56,031 - INFO - Loading and setting up policy model...
2025-11-30 17:54:29,167 - INFO - Loading Cosmos QA dataset...
2025-11-30 17:59:52,222 - INFO - Loading tokenizer and model: meta-llama/Llama-3.2-1B-Instruct
2025-11-30 17:59:53,924 - INFO - Tokenizer lacked pad token; set to eos_token.
2025-11-30 17:59:53,925 - INFO - Loading reference model...
2025-11-30 18:00:01,925 - INFO - Loading and setting up policy model...
2025-11-30 18:00:14,365 - INFO - Loading Cosmos QA dataset...
2025-11-30 18:09:48,749 - INFO - Loading tokenizer and model: meta-llama/Llama-3.2-1B-Instruct
2025-11-30 18:09:52,154 - INFO - Tokenizer lacked pad token; set to eos_token.
2025-11-30 18:09:52,157 - INFO - Loading reference model...
2025-11-30 18:10:01,120 - INFO - Loading and setting up policy model...
2025-11-30 18:10:13,182 - INFO - Loading Cosmos QA dataset...
2025-11-30 18:12:50,892 - INFO - Train samples: 2701 | Val samples: 299
2025-11-30 18:12:50,952 - INFO - Starting training for 2 epochs
2025-11-30 18:12:50,976 - INFO - Total optimizer steps: 676 (warmup: 67)
2025-11-30 18:12:51,003 - INFO - Epoch 1/2 -- 1351 mini-batches
